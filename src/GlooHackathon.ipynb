{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQrXfqO9BflB",
        "outputId": "159638dc-da07-4a41-8175-536a6d798d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch [1/50], Loss: 0.6201\n",
            "Epoch [2/50], Loss: 0.4569\n",
            "Epoch [3/50], Loss: 0.3715\n",
            "Epoch [4/50], Loss: 0.3630\n",
            "Epoch [5/50], Loss: 0.3660\n",
            "Epoch [6/50], Loss: 0.3608\n",
            "Epoch [7/50], Loss: 0.3563\n",
            "Epoch [8/50], Loss: 0.3612\n",
            "Epoch [9/50], Loss: 0.3597\n",
            "Epoch [10/50], Loss: 0.3695\n",
            "Epoch [11/50], Loss: 0.3596\n",
            "Epoch [12/50], Loss: 0.3596\n",
            "Epoch [13/50], Loss: 0.3652\n",
            "Epoch [14/50], Loss: 0.3733\n",
            "Epoch [15/50], Loss: 0.3563\n",
            "Epoch [16/50], Loss: 0.3691\n",
            "Epoch [17/50], Loss: 0.3595\n",
            "Epoch [18/50], Loss: 0.3653\n",
            "Epoch [19/50], Loss: 0.3597\n",
            "Epoch [20/50], Loss: 0.3557\n",
            "Epoch [21/50], Loss: 0.3567\n",
            "Epoch [22/50], Loss: 0.3643\n",
            "Epoch [23/50], Loss: 0.3603\n",
            "Epoch [24/50], Loss: 0.3560\n",
            "Epoch [25/50], Loss: 0.3557\n",
            "Epoch [26/50], Loss: 0.3554\n",
            "Epoch [27/50], Loss: 0.3604\n",
            "Epoch [28/50], Loss: 0.3646\n",
            "Epoch [29/50], Loss: 0.3609\n",
            "Epoch [30/50], Loss: 0.3655\n",
            "Epoch [31/50], Loss: 0.3694\n",
            "Epoch [32/50], Loss: 0.3577\n",
            "Epoch [33/50], Loss: 0.3660\n",
            "Epoch [34/50], Loss: 0.3591\n",
            "Epoch [35/50], Loss: 0.3645\n",
            "Epoch [36/50], Loss: 0.3648\n",
            "Epoch [37/50], Loss: 0.3656\n",
            "Epoch [38/50], Loss: 0.3655\n",
            "Epoch [39/50], Loss: 0.3607\n",
            "Epoch [40/50], Loss: 0.3651\n",
            "Epoch [41/50], Loss: 0.3646\n",
            "Epoch [42/50], Loss: 0.3655\n",
            "Epoch [43/50], Loss: 0.3634\n",
            "Epoch [44/50], Loss: 0.3632\n",
            "Epoch [45/50], Loss: 0.3561\n",
            "Epoch [46/50], Loss: 0.3554\n",
            "Epoch [47/50], Loss: 0.3614\n",
            "Epoch [48/50], Loss: 0.3692\n",
            "Epoch [49/50], Loss: 0.3656\n",
            "Epoch [50/50], Loss: 0.3648\n",
            "Accuracy: 88.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-a6a599df3dee>:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/OregonChurches/church_longevity_model.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib  # For saving and loading the label encoders and scaler\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive (if using Google Colab)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File paths\n",
        "old_churches_1 = '/content/drive/MyDrive/OregonChurches/Old_Churches_Oregon_1.csv'\n",
        "old_churches_2 = '/content/drive/MyDrive/OregonChurches/Old_Churches_Oregon_2.csv'\n",
        "old_churches_3 = '/content/drive/MyDrive/OregonChurches/Old_Churches_Oregon_3.csv'\n",
        "latest_churches = '/content/drive/MyDrive/OregonChurches/Churches_Latest.csv'\n",
        "\n",
        "# Load the data\n",
        "df_old_1 = pd.read_csv(old_churches_1)\n",
        "df_old_2 = pd.read_csv(old_churches_2)\n",
        "df_old_3 = pd.read_csv(old_churches_3)\n",
        "\n",
        "# Clean column names\n",
        "df_old_1.columns = df_old_1.columns.str.strip()\n",
        "df_old_2.columns = df_old_2.columns.str.strip()\n",
        "df_old_3.columns = df_old_3.columns.str.strip()\n",
        "\n",
        "# Define common columns\n",
        "common_columns = ['Business Name', 'Entity Type', 'Nonprofit Type', 'Registry Date',\n",
        "                  'Associated Name Type', 'Address', 'City', 'State', 'Zip Code']\n",
        "\n",
        "# Filter dataframes to common columns\n",
        "common_columns_1 = [col for col in common_columns if col in df_old_1.columns]\n",
        "common_columns_2 = [col for col in common_columns if col in df_old_2.columns]\n",
        "common_columns_3 = [col for col in common_columns if col in df_old_3.columns]\n",
        "\n",
        "df_old_1_filtered = df_old_1[common_columns_1]\n",
        "df_old_2_filtered = df_old_2[common_columns_2]\n",
        "df_old_3_filtered = df_old_3[common_columns_3]\n",
        "\n",
        "# Merge old church data\n",
        "df_old_merged = pd.concat([df_old_1_filtered, df_old_2_filtered, df_old_3_filtered], ignore_index=True)\n",
        "\n",
        "# Load latest church data\n",
        "df_latest = pd.read_csv(latest_churches)\n",
        "df_latest.columns = df_latest.columns.str.strip()\n",
        "\n",
        "# Rename columns in latest data to match\n",
        "df_latest_renamed = df_latest.rename(columns={\n",
        "    'NAME': 'Business Name',\n",
        "    'STREET': 'Address',\n",
        "    'CITY': 'City',\n",
        "    'STATE': 'State',\n",
        "    'ZIP': 'Zip Code',\n",
        "})\n",
        "\n",
        "# Filter latest data to common columns\n",
        "df_latest_filtered = df_latest_renamed[[col for col in common_columns if col in df_latest_renamed.columns]]\n",
        "\n",
        "# Filter to only churches\n",
        "df_latest_filtered = df_latest_filtered[df_latest_filtered['Business Name'].str.contains('church', case=False, na=False)]\n",
        "\n",
        "# Save the latest filtered data\n",
        "df_latest_filtered_file = '/content/drive/MyDrive/OregonChurches/Churches_Latest.csv'\n",
        "df_latest_filtered.to_csv(df_latest_filtered_file, index=False)\n",
        "\n",
        "# Determine remaining churches and churches that no longer exist\n",
        "remaining_churches = df_old_merged[df_old_merged['Business Name'].isin(df_latest_filtered['Business Name'])]\n",
        "remaining_churches_cleaned = remaining_churches.drop_duplicates(subset=['Business Name'])\n",
        "remaining_churches_file = '/content/drive/MyDrive/OregonChurches/Remaining_Churches_Cleaned.csv'\n",
        "remaining_churches_cleaned.to_csv(remaining_churches_file, index=False)\n",
        "\n",
        "churches_no_longer_exist = df_old_merged[~df_old_merged['Business Name'].isin(df_latest_filtered['Business Name'])]\n",
        "churches_no_longer_exist_cleaned = churches_no_longer_exist.drop_duplicates(subset=['Business Name'])\n",
        "no_longer_exist_file = '/content/drive/MyDrive/OregonChurches/Churches_No_Longer_Exist_Cleaned.csv'\n",
        "churches_no_longer_exist_cleaned.to_csv(no_longer_exist_file, index=False)\n",
        "\n",
        "# Read the cleaned data\n",
        "remaining_churches = pd.read_csv('/content/drive/MyDrive/OregonChurches/Remaining_Churches_Cleaned.csv')\n",
        "churches_no_longer_exist = pd.read_csv('/content/drive/MyDrive/OregonChurches/Churches_No_Longer_Exist_Cleaned.csv')\n",
        "\n",
        "# Add 'Longevity' label\n",
        "remaining_churches['Longevity'] = 1\n",
        "churches_no_longer_exist['Longevity'] = 0\n",
        "\n",
        "# Combine the data\n",
        "df = pd.concat([remaining_churches, churches_no_longer_exist], ignore_index=True)\n",
        "\n",
        "# Use separate label encoders for 'City' and 'State'\n",
        "label_encoder_city = LabelEncoder()\n",
        "df['City'] = df['City'].astype(str)  # Ensure 'City' is of type str\n",
        "df['City_encoded'] = label_encoder_city.fit_transform(df['City'])\n",
        "\n",
        "label_encoder_state = LabelEncoder()\n",
        "df['State'] = df['State'].astype(str)  # Ensure 'State' is of type str\n",
        "df['State_encoded'] = label_encoder_state.fit_transform(df['State'])\n",
        "\n",
        "# Save the label encoders\n",
        "joblib.dump(label_encoder_city, '/content/drive/MyDrive/OregonChurches/label_encoder_city.pkl')\n",
        "joblib.dump(label_encoder_state, '/content/drive/MyDrive/OregonChurches/label_encoder_state.pkl')\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['City_encoded', 'State_encoded', 'Zip Code']]\n",
        "y = df['Longevity']\n",
        "\n",
        "# Split into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/OregonChurches/scaler.pkl')\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define the neural network model\n",
        "class ChurchLongevityNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChurchLongevityNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train_tensor.shape[1], 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ChurchLongevityNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/OregonChurches/church_longevity_model.pth')\n",
        "\n",
        "# Inference function\n",
        "def preprocess_input(city, state, zipcode, label_encoder_city, label_encoder_state, scaler):\n",
        "    # Encode 'City' and 'State'\n",
        "    city = str(city)\n",
        "    state = str(state)\n",
        "    try:\n",
        "        city_encoded = label_encoder_city.transform([city])[0]\n",
        "    except ValueError:\n",
        "        print(f\"City '{city}' not found in label encoder. Using default encoding.\")\n",
        "        city_encoded = -1  # Assign a default value\n",
        "\n",
        "    try:\n",
        "        state_encoded = label_encoder_state.transform([state])[0]\n",
        "    except ValueError:\n",
        "        print(f\"State '{state}' not found in label encoder. Using default encoding.\")\n",
        "        state_encoded = -1  # Assign a default value\n",
        "\n",
        "    # Prepare the input array\n",
        "    input_array = np.array([[city_encoded, state_encoded, int(zipcode)]], dtype=np.float32)\n",
        "\n",
        "    # Handle default encoding case\n",
        "    # Replace -1 with mean encoding from training data (optional)\n",
        "    if city_encoded == -1:\n",
        "        input_array[0, 0] = X['City_encoded'].mean()\n",
        "    if state_encoded == -1:\n",
        "        input_array[0, 1] = X['State_encoded'].mean()\n",
        "\n",
        "    # Scale the input\n",
        "    input_scaled = scaler.transform(input_array)\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "def predict_from_input(city, state, zipcode, model, label_encoder_city, label_encoder_state, scaler):\n",
        "    # Preprocess the input\n",
        "    input_tensor = preprocess_input(city, state, zipcode, label_encoder_city, label_encoder_state, scaler)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        prediction = output.item()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Load the label encoders and scaler\n",
        "label_encoder_city = joblib.load('/content/drive/MyDrive/OregonChurches/label_encoder_city.pkl')\n",
        "label_encoder_state = joblib.load('/content/drive/MyDrive/OregonChurches/label_encoder_state.pkl')\n",
        "scaler = joblib.load('/content/drive/MyDrive/OregonChurches/scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = ChurchLongevityNN()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/OregonChurches/church_longevity_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city = input(\"Enter City: \")\n",
        "zipcode = input(\"Enter ZIP code: \")\n",
        "\n",
        "result = predict_from_input(city, \"OR\", zipcode, model, label_encoder_city, label_encoder_state, scaler)\n",
        "\n",
        "print(f'Prediction for {city}, ZIP code {zipcode}: After 8 years, your church has the chance of {result} staying in business.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHxdjNgo9of8",
        "outputId": "fa01b32f-7f5a-4c7e-ec82-25ffda336932"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter City: SPRINGFIELD\n",
            "Enter ZIP code: 97477\n",
            "Prediction for SPRINGFIELD, ZIP code 97477: After 8 years, your church has the chance of 0.18564321100711823 staying in business.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}